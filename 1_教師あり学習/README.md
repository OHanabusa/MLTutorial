# 教師あり学習
ここではニューラルネットワークを用いた教師あり学習の理論を紹介する．その後，理論とプログラムを比較しながら実際にプログラムを動かし，学習が行われる様子を体験してもらう．
# 教師あり学習の概要
まず，教師あり学習では，入力データとその正解のラベルが分かっている必要がある．例えば，猫と犬の画像からどちらか認識するニューラルネットワークを学習するときは，それぞれの画像に正解データとして犬または猫という情報が紐づいている必要がある．

ニューラルネットワークを用いた教師あり学習における学習の手順は以下の通りである．
1. __順伝播（Forward Propagation）__：ニューラルネットワークにおいて情報を入力した際，入力層から出力層へ順方向として出力が計算される．各層の出力は，入力とバイアス・重み行列と呼ばれる変数を行列計算し，活性化関数に入力することで計算される．
2. __誤差の計算__：損失関数を用いて，出力と正解の誤差が計算される．また，損失関数は学習モデルの性能を評価するための関数でもある．
3. __逆伝播（Backward Propagation）__：出力層から入力層に向かって順伝搬とは逆方向に，各層の誤差が計算される．各層の誤差は，その層の重み行列と活性化関数の勾配に基づいて計算される．さらに，各ノードの誤差は，直前の層に伝播される
4.  __勾配の計算__：各層での誤差を用いて，各バイアス・重みの勾配が計算される．勾配は，損失関数を各重みで偏微分することで計算される．この際，連鎖則を用いて，誤差がどのように前の層に伝播するか計算される．
5.  __バイアス・重みの更新__：勾配降下法やその変種を用いて，各バイアス・重みが更新される．

以上の手順を各入力データに対して行うことによって，学習が進み，出力が正解に近づく．
# 教師あり学習の理論
## ニューラルネットワークの構造
<p align="center">
  <img src="https://github.com/SolidMechanicsGroup/ML_Tutorial_2024/assets/130419605/34174a6b-9629-46f9-874c-8fd25867b128">
</p>

ニューラルネットワークは入力層，出力層，そしてその間にある複数の隠れ層から構成される．入力層と出力層は問題に応じて必要なノード数を決める．隠れ層のノード数は自由に決めることができる．上図に示すように，入力層には入力ベクトル，入力ベクトルから全結合された隠れ層には隠れ層ベクトル，隠れ層ベクトルから全結合された出力層には出力層ベクトルが各層のノードの情報となる．
入力層には入力ベクトル  $x$  の次元数のノードが存在し，各要素の実数を保持する．
隠れ層や出力層の各層には，複数のノードが存在する．このとき，第 $l$ 層の $i$ 番目のノードを $h_i^{(l)}$ と表記する．
各ノード $h_i^{(l)}$ は，隠れ層の初めの層を1，出力層を $L$ とする層番号 $l(=1,2,...,L)$ ，第 $l-1$ 層の $i$ 番目のノードと第 $l$ 層の $j$ 番目のノードを結ぶ重み行列 $W_{ij}^{(l)}$ ，第 $l$ 層の $i$ 番目のノードのバイアス項 $b_i^{(l)}$ ，第 $l$ 層の活性化関数 $f^{(l)}$ (ReLU, tanhなど)を用いて，以下のように定義される．

$$
\begin{align}
    h_i^{(l)}&=f^{(l)}I_i^{(l)} \\
    I_i^{(l)}&=\sum_j W_{ij}^{(l)} h_j^{(l-1)}+b_i^{(l)}
\end{align}
$$

したがって，各ノードでは，1つ前の層の出力と重み行列の積の合計にバイアスを加え，活性化関数を通して，各ノードの値が計算される．
この際の重み行列 $W^{(l)}$ やバイアスベクトル $b^{(l)}$ がニューラルネットワークの学習パラメータとなる．また，第 $l$ 層目の重み行列 $W^{(l)}$ とバイアスベクトル $b^{(l)}$ の初期値は，一般的に，第 $l-1$ 層目のノード数 $d$ に対して， $\left(-\frac{1}{\sqrt{d}}, \frac{1}{\sqrt{d}}\right)$ の範囲でランダムに設定される．

## 学習の流れ
[英の卒論](https://github.com/SolidMechanicsGroup/ML_Tutorial_2024/blob/33ce72255cbbb695ec96588a9e1aa9ab11727390/%E5%8D%92%E6%A5%AD%E8%AB%96%E6%96%87_%E8%8B%B1%E9%9F%B3.pdf)
[誤差逆伝搬法](https://qiita.com/43x2/items/50b55623c890564f1893#%E4%B8%80%E8%88%AC%E5%8C%96)
[Cross entropy loss](https://qiita.com/kenta1984/items/59a9ef1788e6934fd962)
