ここではニューラルネットワークを用いた教師あり学習の理論を紹介する．その後，理論とプログラムを比較しながら実際にプログラムを動かし，学習が行われる様子を体験してもらう．
# 教師あり学習の概要
まず，教師あり学習では，入力データとその正解のラベルが分かっている必要がある．例えば，猫と犬の画像からどちらか認識するニューラルネットワークを学習するときは，それぞれの画像に正解データとして犬または猫という情報が紐づいている必要がある．

ニューラルネットワークを用いた教師あり学習における学習の手順は以下の通りである．
1. __順伝播（Forward Propagation）__：ニューラルネットワークにおいて情報を入力した際，入力層から出力層へ順方向として出力が計算される．各層の出力は，入力とバイアス・重み行列と呼ばれる変数を行列計算し，活性化関数に入力することで計算される．
2. __誤差の計算__：損失関数を用いて，出力と正解の誤差が計算される．また，損失関数は学習モデルの性能を評価するための関数でもある．
3. __逆伝播（Backward Propagation）__：出力層から入力層に向かって順伝搬とは逆方向に，各層の誤差が計算される．各層の誤差は，その層の重み行列と活性化関数の勾配に基づいて計算される．さらに，各ノードの誤差は，直前の層に伝播される
4.  __勾配の計算__：各層での誤差を用いて，各バイアス・重みの勾配が計算される．勾配は，損失関数を各重みで偏微分することで計算される．この際，連鎖則を用いて，誤差がどのように前の層に伝播するか計算される．
5.  __バイアス・重みの更新__：勾配降下法やその変種を用いて，各バイアス・重みが更新される．

以上の手順を各入力データに対して行うことによって，学習が進み，出力が正解に近づく．
# 教師あり学習の理論
## ニューラルネットワークの構造
<p align="center">
  <img src="https://github.com/SolidMechanicsGroup/ML_Tutorial_2024/assets/130419605/34174a6b-9629-46f9-874c-8fd25867b128">
</p>

ニューラルネットワークは入力層，出力層，そしてその間にある複数の隠れ層から構成される．入力層と出力層は問題に応じて必要なノード数を決める．隠れ層のノード数は自由に決めることができる．上図に示すように，入力層には入力ベクトル，入力ベクトルから全結合された隠れ層には隠れ層ベクトル，隠れ層ベクトルから全結合された出力層には出力層ベクトルが各層のノードの情報となる．
入力層には入力ベクトル  $x$  の次元数のノードが存在し，各要素の実数を保持する．
隠れ層や出力層の各層には，複数のノードが存在する．このとき，第 $l$ 層の $i$ 番目のノードを $h_i^{(l)}$ と表記する．
各ノード $h_i^{(l)}$ は，隠れ層の初めの層を1，出力層を $L$ とする層番号 $l(=1,2,...,L)$ ，第 $l-1$ 層の $i$ 番目のノードと第 $l$ 層の $j$ 番目のノードを結ぶ重み行列 $W_{ij}^{(l)}$ ，第 $l$ 層の $i$ 番目のノードのバイアス項 $b_i^{(l)}$ ，第 $l$ 層の活性化関数 $f^{(l)}$ (ReLU, tanhなど)を用いて，以下のように定義される．

$$
\begin{align}
    h_i^{(l)}&=f^{(l)}I_i^{(l)} \\
    I_i^{(l)}&=\sum_j W_{ij}^{(l)} h_j^{(l-1)}+b_i^{(l)}
\end{align}
$$

したがって，各ノードでは，1つ前の層の出力と重み行列の積の合計にバイアスを加え，活性化関数を通して，各ノードの値が計算される．
この際の重み行列 $W^{(l)}$ やバイアスベクトル $b^{(l)}$ がニューラルネットワークの学習パラメータとなる．また，第 $l$ 層目の重み行列 $W^{(l)}$ とバイアスベクトル $b^{(l)}$ の初期値は，一般的に，第 $l-1$ 層目のノード数 $d$ に対して， $\left(-\frac{1}{\sqrt{d}}, \frac{1}{\sqrt{d}}\right)$ の範囲でランダムに設定される．

## 活性化関数
活性化関数は，非線形な関数の学習を可能にするという役割を果たす，ニューラルネットワークにおける重要な構成要素である．そのため，活性化関数を取り入れることでニューラルネットワークの表現力を向上させることができる．また，活性化関数には，いくつか種類があり，代表的なものとしてステップ関数，sigmoid関数，ReLU関数などが挙げられる．これらはその関数形や学習するデータに応じて適切な役割が決まっており，用途によって使い分けられる．一般的に，隠れ層の活性化関数はReLU関数が使われ，出力層の活性化関数は得たい出力に応じて決める．-1～1の範囲の出力が欲しいのならばtanh，0～1の範囲であればsigmoidという感じである．

例として，tanhとLeaky ReLU関数を以下に示す．

$$
\begin{align}
    f_{\tanh} &=  \frac{e^{2x} - 1}{e^{2x} + 1}\\
    f_{\mathrm{Leaky ReLU}} &= \begin{cases}
    x, & \text{if } x > 0 \\
    \alpha x, & \text{if } x \leq 0
    \end{cases}
\end{align}
$$

ここで， $\alpha$ はLeaky ReLUの傾きを示し，一般的に0.01とされる．これらの関数をプロットした様子を下図に示す．図に示すように，tanhは出力が-1から1の範囲に限定されることが分かる．また， $x=0$ 付近で勾配が大きいため学習効率が高いが， $x=0$ から離れた領域では勾配がゼロになるという特徴がある．Leaky ReLUは $x<0$ において出力をゼロとするReLU関数の変種である．そのため， $x<0$ 領域において微小な傾きを持ち，勾配がゼロにならないという特徴がある．
<p align="center">
  <img src="https://github.com/SolidMechanicsGroup/ML_Tutorial_2024/assets/130419605/652cbb6d-ad5b-41b2-ba7e-4edffd02d8db" width="75%">
</p>

その他の活性化関数は参考資料のサイトまたは各自で調べてみてください．活性化関数は出力の範囲を定めるだけでなく，活性化関数の勾配が学習を行う際にとても重要な要素となっているのでいろいろ試してみてください．

## 誤差関数
損失関数（loss function）は，機械学習モデルがどれくらい正しく予測できているか評価する指標です．また，ニューラルネットワークの学習において損失関数を最小化するようにパラメータを更新します．損失関数は任意の関数を用いることができますが，クラス分類や値の予想などの問題によってそれぞれ一般的に使われる関数があります．最もよく使われる関数が2乗和誤差と呼ばれ，以下のように定義される．

$$
E =  \sum_i \frac{1}{2}({y}_i - {t}_i)^2
$$

## 誤差逆伝搬法


## 補足資料
[英の卒論](https://github.com/SolidMechanicsGroup/ML_Tutorial_2024/blob/33ce72255cbbb695ec96588a9e1aa9ab11727390/%E5%8D%92%E6%A5%AD%E8%AB%96%E6%96%87_%E8%8B%B1%E9%9F%B3.pdf)

[活性化関数](https://nisshingeppo.com/ai/activation-functions-list/)

[誤差逆伝搬法](https://qiita.com/43x2/items/50b55623c890564f1893#%E4%B8%80%E8%88%AC%E5%8C%96)

[Cross entropy loss](https://qiita.com/kenta1984/items/59a9ef1788e6934fd962)
